{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> STS 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "import logging\n",
    "from datetime import datetime\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import DataLoader\n",
    "from datasets import load_dataset\n",
    "from sentence_transformers import SentenceTransformer,  LoggingHandler, losses, models, util\n",
    "from sentence_transformers.evaluation import EmbeddingSimilarityEvaluator\n",
    "from sentence_transformers.readers import InputExample"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import os\n",
    "os.environ['KMP_DUPLICATE_LIB_OK']='True'\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습 경과를 지켜보는데 사용될 logger 를 초기화\n",
    "logging.basicConfig(\n",
    "    format=\"%(asctime)s - %(message)s\",\n",
    "    datefmt=\"%Y-%m-%d %H:%M:%S\",\n",
    "    level=logging.INFO,\n",
    "    handlers=[LoggingHandler()],\n",
    ")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* https://huggingface.co/klue : 다양한 사전학습 언어 모델"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name = \"klue/roberta-base\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_batch_size = 32\n",
    "num_epochs = 4\n",
    "model_save_path = \"output/training_klue_sts_\" + model_name.replace(\"/\", \"-\") + \"-\" + datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaModel: ['lm_head.bias', 'lm_head.decoder.bias', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.dense.weight']\n",
      "- This IS expected if you are initializing RobertaModel from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaModel from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n"
     ]
    }
   ],
   "source": [
    "embedding_model = models.Transformer(model_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Mean Pooling 사용 \n",
    "# 모델이 반환한 모든 토큰 임베딩을 더해준 후, 더해진 토큰 개수만큼 나누어 문장을 대표하는 임베딩으로 사용하는 기법\n",
    "pooler = models.Pooling(\n",
    "    embedding_model.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True,\n",
    "    pooling_mode_cls_token=False,\n",
    "    pooling_mode_max_tokens=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-07 12:41:57 - Use pytorch device: cuda\n"
     ]
    }
   ],
   "source": [
    "model = SentenceTransformer(modules=[embedding_model, pooler])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 사용될 KLUE STS 데이터셋 (훈련, 검증 데이터만 존재)\n",
    "datasets = load_dataset(\"klue\", \"sts\")\n",
    "\n",
    "# KorSTS 데이터셋\n",
    "# 두 데이터셋은 제작 과정이 엄밀히 다르므로, \n",
    "# KLUE STS 데이터에 대해 학습된 모델이 \n",
    "# KorSTS 테스트셋에 대해 기록하는 점수은 \n",
    "# 사실상 큰 의미가 없을 수 있습니다. \n",
    "# 전체적인 훈련 프로세스의 이해를 돕기 위해 사용\n",
    "testsets = load_dataset(\"kor_nlu\", \"sts\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋을 sentence-transformers 훈련 양식에 맞게 변환해주는 작업\n",
    "\n",
    "train_samples = []\n",
    "dev_samples = []\n",
    "test_samples = []\n",
    "\n",
    "# KLUE STS 내 훈련, 검증 데이터 예제 변환\n",
    "for phase in [\"train\", \"validation\"]:\n",
    "    examples = datasets[phase]\n",
    "\n",
    "    for example in examples:\n",
    "        score = float(example[\"labels\"][\"label\"]) / 5.0  # 0.0 ~ 1.0 스케일로 유사도 정규화\n",
    "\n",
    "        inp_example = InputExample(\n",
    "            texts=[example[\"sentence1\"], example[\"sentence2\"]], \n",
    "            label=score,\n",
    "        )\n",
    "\n",
    "        if phase == \"validation\":\n",
    "            dev_samples.append(inp_example)\n",
    "        else:\n",
    "            train_samples.append(inp_example)\n",
    "\n",
    "# KorSTS 내 테스트 데이터 예제 변환\n",
    "for example in testsets[\"test\"]:\n",
    "    score = float(example[\"score\"]) / 5.0\n",
    "\n",
    "    if example[\"sentence1\"] and example[\"sentence2\"]:\n",
    "        inp_example = InputExample(\n",
    "            texts=[example[\"sentence1\"], example[\"sentence2\"]],\n",
    "            label=score,\n",
    "        )\n",
    "\n",
    "    test_samples.append(inp_example)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2023-06-07 12:44:52 - Warmup-steps: 146\n"
     ]
    }
   ],
   "source": [
    "# 학습에 사용될 DataLoader와 Loss를 설정\n",
    "train_dataloader = DataLoader(\n",
    "    train_samples,\n",
    "    shuffle=True,\n",
    "    batch_size=train_batch_size,\n",
    ")\n",
    "train_loss = losses.CosineSimilarityLoss(model=model)\n",
    "\n",
    "# 모델 검증에 활용할 Evaluator 를 정의\n",
    "evaluator = EmbeddingSimilarityEvaluator.from_input_examples(\n",
    "    dev_samples,\n",
    "    name=\"sts-dev\",\n",
    ")\n",
    "\n",
    "# Warm up Steps를 설정\n",
    "warmup_steps = math.ceil(len(train_dataloader) * num_epochs  * 0.1)  # 10% of train data for warm-up\n",
    "logging.info(f\"Warmup-steps: {warmup_steps}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련\n",
    "model.fit(\n",
    "    train_objectives=[(train_dataloader, train_loss)],\n",
    "    evaluator=evaluator,\n",
    "    epochs=num_epochs,\n",
    "    evaluation_steps=1000,\n",
    "    warmup_steps=warmup_steps,\n",
    "    output_path=model_save_path,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch, gc\n",
    "gc.collect()\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> NLI 학습"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "import logging\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import datasets\n",
    "from datasets import load_dataset, load_metric, ClassLabel, Sequence\n",
    "from transformers import AutoTokenizer, AutoModelForSequenceClassification, TrainingArguments, Trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model_checkpoint = \"klue/roberta-base\"\n",
    "batch_size = 32\n",
    "task = \"nli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "datasets = load_dataset(\"klue\", task)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 훈련 과정 중 모델의 성능을 파악하기 위한 메트릭을 설정\n",
    "metric = load_metric(\"glue\", \"qnli\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습에 활용할 토크나이저를 로드\n",
    "tokenizer = AutoTokenizer.from_pretrained(model, use_fast=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 로드한 데이터셋에서 각 문장에 해당하는 value 를 뽑아주기 위한 key 를 정의\n",
    "sentence1_key, sentence2_key = (\"premise\", \"hypothesis\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 데이터셋에서 각 예제들을 뽑아와 토큰화 할 수 있는 함수\n",
    "def preprocess_function(examples):\n",
    "    return tokenizer(\n",
    "        examples[sentence1_key],\n",
    "        examples[sentence2_key],\n",
    "        truncation=True,\n",
    "        return_token_type_ids=False,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 전처리 함수를 활용해 데이터셋을 미리 토큰화시키는 작업\n",
    "encoded_datasets = datasets.map(preprocess_function, batched=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 학습을 위한 모델을 로드\n",
    "num_labels = 3\n",
    "model = AutoModelForSequenceClassification.from_pretrained(model, num_labels=num_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 앞서 정의한 메트릭을 모델 예측 결과에 적용하기 위한 함수\n",
    "def compute_metrics(eval_pred):\n",
    "    predictions, labels = eval_pred\n",
    "    predictions = np.argmax(predictions, axis=1)\n",
    "    return metric.compute(predictions=predictions, references=labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# transformers에서 제공하는 Trainer 객체를 활용하기 위한 인자 관리 클래스를 초기화\n",
    "metric_name = \"accuracy\"\n",
    "\n",
    "args = TrainingArguments(\n",
    "    \"test-nli\",\n",
    "    # evaluation_strategy=\"epoch\",\n",
    "    evaluation_strategy = \"no\",\n",
    "    save_strategy = 'no',\n",
    "    learning_rate=2e-5,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=metric_name,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model,\n",
    "    args,\n",
    "    train_dataset=encoded_datasets[\"train\"],\n",
    "    eval_dataset=encoded_datasets[\"validation\"],\n",
    "    tokenizer=tokenizer,\n",
    "    compute_metrics=compute_metrics,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 훈련\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer.evaluate()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> 훈련 모델 허깅페이스에 올리기"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# repo\n",
    "my_token = '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'   # https://huggingface.co/settings/token에서 내 토큰 찾아서 입력\n",
    "MODEL_SAVE_REPO = 'yngkyng/klue-roberta-large-sts-ko'\n",
    "HUGGINGFACE_AUTH_TOKEN = my_token\n",
    "\n",
    "# Push to huggingface-hub\n",
    "model.push_to_hub(\n",
    "MODEL_SAVE_REPO,\n",
    "use_temp_dir=False,\n",
    "use_auth_token=HUGGINGFACE_AUTH_TOKEN\n",
    ")\n",
    "\n",
    "tokenizer.push_to_hub(\n",
    "MODEL_SAVE_REPO,\n",
    "use_temp_dir=False,\n",
    "use_auth_token=HUGGINGFACE_AUTH_TOKEN\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
